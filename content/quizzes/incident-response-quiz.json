{
  "id": "incident-response-quiz",
  "title": "Production Incident Response Quiz",
  "description": "Test your skills handling real production outages, performance issues, and system failures under pressure",
  "category": "Incident Response",
  "icon": "AlertTriangle",
  "totalPoints": 130,
  "theme": {
    "primaryColor": "red",
    "gradientFrom": "from-red-500",
    "gradientTo": "to-orange-600"
  },
  "metadata": {
    "estimatedTime": "18-22 minutes",
    "difficultyLevels": {
      "beginner": 3,
      "intermediate": 4,
      "advanced": 3
    },
    "createdDate": "2024-06-20"
  },
  "questions": [
    {
      "id": "database-connection-crisis",
      "title": "Database Connection Pool Exhausted",
      "description": "Your e-commerce site is down during Black Friday sales. Users can't check out.",
      "situation": "It's 2 PM on Black Friday, traffic is 10x normal, and your application is throwing database connection errors. Customers are abandoning carts and revenue is dropping fast.",
      "codeExample": "ERROR logs:\n[ERROR] HikariPool-1 - Connection is not available, request timed out after 30000ms\n[ERROR] Could not obtain connection from pool within 30 seconds\n[ERROR] Database connection pool exhausted\n\nMonitoring shows:\n- CPU: 25% (normal)\n- Memory: 60% (normal) \n- Active DB connections: 100/100 (maxed out)\n- Response time: 15+ seconds",
      "options": [
        "Restart the database server immediately",
        "Scale up the database instance to a larger size",
        "Increase connection pool size, add read replicas, and implement connection timeout tuning",
        "Enable database query caching and hope it helps"
      ],
      "correctAnswer": 2,
      "explanation": "Connection pool exhaustion during traffic spikes requires immediate pool tuning and load distribution. Increasing pool size provides immediate relief, read replicas distribute load, and proper timeouts prevent connection leaks.",
      "hint": "Think about immediate relief for the connection bottleneck and load distribution strategies.",
      "difficulty": "beginner",
      "points": 10
    },
    {
      "id": "memory-leak-investigation",
      "title": "Application Memory Leak",
      "description": "Your Node.js API server memory usage keeps growing until it crashes.",
      "situation": "Every 6 hours, your API servers run out of memory and restart. This started after yesterday's deployment. Memory usage grows from 500MB to 8GB over time.",
      "codeExample": "Symptoms:\n- Memory usage: 500MB → 2GB → 4GB → 8GB → OOM kill\n- Heap dump shows growing object references\n- Restart cycle: Every 6 hours\n- Started after: v2.3.1 deployment yesterday\n\nMonitoring data:\n- Request rate: Normal (1000 req/min)\n- CPU: Normal (30%)\n- Memory: Growing continuously\n- GC frequency: Increasing",
      "options": [
        "Rollback to previous version, analyze heap dumps, and identify the memory leak in recent code changes",
        "Add more memory to the servers",
        "Restart servers every 4 hours with a cron job",
        "Scale horizontally by adding more server instances"
      ],
      "correctAnswer": 0,
      "explanation": "Memory leaks require root cause analysis. Rollback provides immediate relief for users, heap dump analysis identifies the leak source, and reviewing recent changes helps pinpoint the problematic code.",
      "hint": "The leak started with a recent deployment. What's the fastest way to restore service while investigating?",
      "difficulty": "intermediate",
      "points": 15
    },
    {
      "id": "load-balancer-failure",
      "title": "Load Balancer Health Check Failures",
      "description": "Your load balancer is marking healthy servers as unhealthy, causing cascading failures.",
      "situation": "Servers are being removed from the load balancer pool one by one. The remaining servers are getting overloaded and also failing health checks, creating a death spiral.",
      "codeExample": "Load Balancer Status:\n- Server 1: UNHEALTHY (removed from pool)\n- Server 2: UNHEALTHY (removed from pool)  \n- Server 3: WARNING (high CPU due to increased load)\n- Server 4: WARNING (high CPU due to increased load)\n\nHealth Check: GET /health\n- Timeout: 5 seconds\n- Expected: HTTP 200\n- Current response time: 8-12 seconds",
      "options": [
        "Disable health checks temporarily until servers recover",
        "Manually add servers back to the pool via load balancer admin",
        "Increase health check timeout, investigate why /health endpoint is slow, and scale up remaining servers",
        "Deploy new servers and replace the failing ones"
      ],
      "correctAnswer": 2,
      "explanation": "Health check failures during load spikes often indicate timeout issues rather than actual server problems. Increasing timeout prevents false negatives, investigating the health endpoint addresses root cause, and scaling provides immediate relief.",
      "hint": "The servers aren't actually unhealthy - they're just slow to respond. How do you fix the health check logic?",
      "difficulty": "intermediate",
      "points": 13
    },
    {
      "id": "dns-propagation-disaster",
      "title": "DNS Resolution Failure",
      "description": "After a DNS change, some users can't reach your site while others can.",
      "situation": "You updated DNS records 2 hours ago to point to new servers. About 30% of users report the site is unreachable, while others see it fine. Support tickets are flooding in.",
      "codeExample": "DNS Change Made:\nmyapp.com A record: 1.2.3.4 → 5.6.7.8 (TTL: 300 seconds)\n\nUser Reports:\n- \"Site not loading\" (30% of users)\n- \"Works fine for me\" (70% of users)\n- Geographic pattern: Issues mostly in Asia/Europe\n\nDig results:\n$ dig myapp.com @8.8.8.8\n;; ANSWER SECTION:\nmyapp.com. 300 IN A 5.6.7.8\n\n$ dig myapp.com @1.1.1.1  \n;; ANSWER SECTION:\nmyapp.com. 300 IN A 1.2.3.4",
      "options": [
        "Wait for DNS propagation to complete (up to 48 hours)",
        "Revert DNS changes immediately and investigate the new servers",
        "Clear DNS cache on all ISPs globally",
        "Keep both old and new servers running, ensure new servers are healthy, and wait for TTL expiration"
      ],
      "correctAnswer": 3,
      "explanation": "During DNS transitions, keep old infrastructure running until propagation completes. Verify new servers are actually working correctly, and let TTL naturally expire rather than forcing changes that could make things worse.",
      "hint": "Users are split between old and new servers. How do you ensure everyone can access the site during the transition?",
      "difficulty": "beginner",
      "points": 12
    },
    {
      "id": "ssl-certificate-expiration",
      "title": "SSL Certificate Expired at Midnight",
      "description": "Your SSL certificate expired overnight and users are getting security warnings.",
      "situation": "It's 6 AM and you're getting reports that users see 'Your connection is not secure' warnings. The SSL certificate expired at midnight, but your monitoring didn't alert you.",
      "codeExample": "Browser Error:\n\"Your connection is not secure\"\n\"NET::ERR_CERT_DATE_INVALID\"\n\nCertificate Info:\n- Domain: api.myapp.com\n- Expired: Today at 00:00:00 UTC\n- Issuer: Let's Encrypt\n- Valid from: 3 months ago\n\nCurrent Status:\n- Website: Inaccessible (security warnings)\n- API: Failing (SSL handshake errors)\n- Mobile app: Can't connect to backend",
      "options": [
        "Disable SSL temporarily and use HTTP until new certificate is ready",
        "Renew SSL certificate immediately, update server configuration, and implement certificate monitoring",
        "Ask users to ignore the security warning and continue",
        "Switch to a different domain name that has a valid certificate"
      ],
      "correctAnswer": 1,
      "explanation": "SSL certificate renewal is critical for security and user trust. Renew immediately using automated tools like certbot, update server configs, and implement monitoring to prevent future expirations.",
      "hint": "Security can't be compromised, but service needs to be restored quickly. What's the proper way to handle expired certificates?",
      "difficulty": "beginner",
      "points": 10
    },
    {
      "id": "disk-space-emergency",
      "title": "Production Server Running Out of Disk Space",
      "description": "Your main application server is at 98% disk usage and applications are starting to fail.",
      "situation": "Log files have grown to fill the disk, new deployments are failing, and the database can't write transaction logs. You need to free space immediately without losing important data.",
      "codeExample": "Disk Usage:\n$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda1        50G   49G  500M  98% /\n\nLargest directories:\n$ du -sh /* | sort -hr | head -5\n25G    /var/log\n15G    /opt/app/uploads\n5G     /tmp\n2G     /var/cache\n1G     /home\n\nCritical Errors:\n- \"No space left on device\" in application logs\n- Database: \"Cannot write to transaction log\"\n- Deployment: \"Failed to extract package\"",
      "options": [
        "Delete everything in /tmp and /var/cache immediately",
        "Add more disk space to the server",
        "Restart the server to clear temporary files",
        "Compress and archive old logs, clean temporary files, move uploads to external storage, and set up log rotation"
      ],
      "correctAnswer": 3,
      "explanation": "Systematic cleanup preserves important data while freeing space. Compress/archive logs for potential debugging, clean safe temporary files, move uploads to scalable storage, and implement rotation to prevent recurrence.",
      "hint": "You need immediate space but can't lose important data. Which approach safely frees the most space while preserving what you might need?",
      "difficulty": "intermediate",
      "points": 14
    },
    {
      "id": "kubernetes-pod-crashloop",
      "title": "Kubernetes Pods in CrashLoopBackOff",
      "description": "After a deployment, all your application pods are crashing and restarting continuously.",
      "situation": "You deployed a new version 30 minutes ago. Pods start, run for 10 seconds, then crash. Kubernetes keeps restarting them but they never become ready.",
      "codeExample": "kubectl get pods:\nNAME                    READY   STATUS             RESTARTS   AGE\napi-deployment-abc123   0/1     CrashLoopBackOff   8          30m\napi-deployment-def456   0/1     CrashLoopBackOff   8          30m\napi-deployment-ghi789   0/1     CrashLoopBackOff   8          30m\n\nkubectl logs api-deployment-abc123:\nStarting application...\nConnecting to database...\nFatal error: Environment variable DB_PASSWORD not found\nExiting with code 1",
      "options": [
        "Rollback deployment, verify ConfigMap/Secret configuration, and redeploy with correct environment variables",
        "Increase pod memory and CPU limits",
        "Scale the deployment to more replicas",
        "Restart the Kubernetes cluster"
      ],
      "correctAnswer": 0,
      "explanation": "CrashLoopBackOff usually indicates configuration issues. Rollback provides immediate service restoration, then investigate missing environment variables, ConfigMaps, or Secrets before attempting redeploy.",
      "hint": "The error message shows a missing environment variable. What's the fastest way to restore service while fixing the configuration?",
      "difficulty": "intermediate",
      "points": 16
    },
    {
      "id": "network-partition-chaos",
      "title": "Network Partition Between Services",
      "description": "Your microservices can't communicate with each other due to network issues.",
      "situation": "The frontend can't reach the user service, the user service can't reach the database, but everything appears to be running. Network timeouts are occurring between different subnets.",
      "codeExample": "Service Status:\n✅ Frontend pods: Running\n✅ User service pods: Running  \n✅ Database: Running\n✅ Load balancers: Healthy\n\nErrors:\n- Frontend → User Service: \"Connection timeout after 30s\"\n- User Service → Database: \"Connection refused\"\n- Ping between subnets: 100% packet loss\n\nInfrastructure:\n- Frontend: Subnet A (10.0.1.0/24)\n- User Service: Subnet B (10.0.2.0/24)\n- Database: Subnet C (10.0.3.0/24)",
      "options": [
        "Restart all the services to reset network connections",
        "Check security groups, NACLs, route tables, and firewall rules between subnets",
        "Increase network timeouts in application configuration",
        "Deploy all services to the same subnet"
      ],
      "correctAnswer": 1,
      "explanation": "Network partitions are usually caused by infrastructure changes to security groups, NACLs, route tables, or firewalls. Systematic checking of network policies between subnets will identify the blocking rule.",
      "hint": "Services are running but can't talk to each other across subnets. What controls traffic between network segments?",
      "difficulty": "advanced",
      "points": 18
    },
    {
      "id": "cascading-failure-storm",
      "title": "Cascading Service Failures",
      "description": "One service failure is triggering a domino effect across your entire system.",
      "situation": "The payment service went down, but now your entire platform is experiencing failures. Services that don't even use payments are timing out and becoming unresponsive.",
      "codeExample": "Failure Timeline:\n14:30 - Payment service: High error rate\n14:32 - Order service: Timeouts calling payments\n14:35 - User service: High CPU (retry storms)\n14:37 - Frontend: 500 errors\n14:40 - Database: Connection pool exhausted\n14:42 - Load balancer: All backends unhealthy\n\nCurrent State:\n- Payment service: DOWN\n- Order service: Degraded (retry loops)\n- User service: Overloaded \n- Database: Unresponsive\n- Frontend: Completely down",
      "options": [
        "Implement circuit breakers, isolate the payment service, enable graceful degradation, and restore services in dependency order",
        "Restart all services simultaneously to reset the system state",
        "Fix the payment service first, then wait for other services to recover",
        "Scale up all services to handle the increased load"
      ],
      "correctAnswer": 0,
      "explanation": "Cascading failures require immediate isolation of the root cause and systematic recovery. Circuit breakers prevent retry storms, graceful degradation maintains partial functionality, and dependency-order recovery prevents re-triggering the cascade.",
      "hint": "One failing service shouldn't bring down everything. How do you stop the domino effect and recover systematically?",
      "difficulty": "advanced",
      "points": 22
    },
    {
      "id": "data-corruption-incident",
      "title": "Database Corruption During Peak Hours",
      "description": "Your primary database is reporting data corruption errors during the busiest time of day.",
      "situation": "It's peak shopping hours and the database is throwing corruption errors. Some user orders are being lost, payment data is inconsistent, and you need to maintain data integrity while keeping the service running.",
      "codeExample": "Database Errors:\n[ERROR] Table 'orders' is marked as crashed and should be repaired\n[ERROR] Incorrect key file for table 'users'; try to repair it\n[ERROR] Got error 127 from storage engine\n\nImpact:\n- Lost orders: ~50 in the last hour\n- Payment mismatches: $15,000 in pending transactions\n- User complaints: Cannot access order history\n- Business impact: $2,000/minute revenue loss\n\nSystem Status:\n- Primary DB: Partial corruption\n- Read replicas: 15 minutes behind\n- Application: Degraded performance",
      "options": [
        "Take the database offline immediately and run full repair",
        "Continue running and fix corruption during maintenance window",
        "Failover to read replica, promote it to primary, run repair on original primary in background",
        "Restore from last night's backup and lose recent data"
      ],
      "correctAnswer": 2,
      "explanation": "Data corruption during peak hours requires immediate failover to preserve business continuity. Promoting a read replica minimizes data loss, allows continued operations, and provides time to properly repair the corrupted primary database offline.",
      "hint": "You need to maintain service while ensuring data integrity. What allows you to keep serving customers while fixing the corruption?",
      "difficulty": "advanced",
      "points": 20
    }
  ]
}
