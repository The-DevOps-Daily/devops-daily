{
  "id": "kubernetes-security",
  "slug": "kubernetes-security",
  "title": "Kubernetes Security Checklist",
  "description": "Essential security checklist for Kubernetes clusters to ensure production readiness.",
  "category": "Cloud",
  "difficulty": "advanced",
  "estimatedTime": "1-2 hours",
  "tags": ["kubernetes", "k8s", "security", "containers", "cloud"],
  "items": [
    {
      "id": "enable-rbac",
      "title": "Enable RBAC",
      "description": "RBAC (Role-Based Access Control) is essential for Kubernetes security. Most modern clusters have RBAC enabled by default, but verify with: 'kubectl api-versions | grep rbac'. If missing, enable in kube-apiserver with '--authorization-mode=RBAC'. Create least-privilege roles: define Role/ClusterRole objects specifying exact resources and verbs (get, list, create, etc.). Bind roles to users/groups with RoleBinding/ClusterRoleBinding. Example: Create a role for viewing pods only: 'kubectl create role pod-reader --verb=get,list,watch --resource=pods'. Bind it to a user: 'kubectl create rolebinding read-pods --role=pod-reader --user=jane'. Never use cluster-admin for regular operations. Audit existing roles: 'kubectl get clusterrolebindings -o wide' and remove overly permissive bindings. Use tools like rbac-lookup or kubectl-who-can to audit permissions.",
      "critical": true
    },
    {
      "id": "configure-network-policies",
      "title": "Configure network policies",
      "description": "By default, all pods can communicate with each other - this is a security risk. NetworkPolicies act as pod-level firewalls. First, ensure your CNI plugin supports NetworkPolicy (Calico, Cilium, Weave work; Flannel alone doesn't). Create a default deny-all policy: 'kubectl apply -f deny-all.yaml' with 'podSelector: {}' and empty ingress/egress. Then create specific allow rules for required communication. Example: Allow frontend pods to talk to backend on port 8080: create NetworkPolicy selecting backend pods, allowing ingress from frontend pods on port 8080. Label pods consistently: 'app: backend, tier: api'. Test policies thoroughly - overly restrictive rules will break applications. Use 'kubectl describe networkpolicy' to verify rules. Tools like Cilium Editor help visualize network flows.",
      "critical": true
    },
    {
      "id": "pod-security-standards",
      "title": "Use Pod Security Standards",
      "description": "Pod Security Standards (PSS) enforce security best practices at the namespace level. Kubernetes 1.23+ has built-in Pod Security Admission replacing deprecated PodSecurityPolicies. Three levels exist: Privileged (unrestricted), Baseline (minimally restrictive, prevents known privilege escalations), and Restricted (heavily restricted, follows pod hardening best practices). Apply to namespaces via labels: 'kubectl label namespace production pod-security.kubernetes.io/enforce=restricted pod-security.kubernetes.io/audit=restricted pod-security.kubernetes.io/warn=restricted'. Restricted mode blocks: privileged containers, host namespaces, host ports, insecure capabilities, root users. Configure exceptions carefully. For older clusters, use Open Policy Agent (OPA) or Kyverno for policy enforcement. Review violations: 'kubectl get events --field-selector reason=FailedCreate'.",
      "critical": false
    },
    {
      "id": "enable-audit-logging",
      "title": "Enable audit logging",
      "description": "Audit logs track all API requests - critical for security investigations and compliance. Create an audit policy file defining what to log. Example policy: log metadata for all requests, request/response bodies for secrets. Place policy at /etc/kubernetes/audit-policy.yaml. Configure kube-apiserver flags: '--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kubernetes/audit.log --audit-log-maxage=30 --audit-log-maxbackup=10 --audit-log-maxsize=100'. This rotates logs, keeps 30 days of history. For production, send logs to external systems (Elasticsearch, Splunk, CloudWatch). Use audit2rbac tool to analyze logs and identify least-privilege permissions. Monitor for suspicious activity: excessive failures, unusual resource access, secret reads. Audit logs are verbose - tune policy to balance detail vs. performance impact.",
      "critical": false
    },
    {
      "id": "scan-images",
      "title": "Scan container images for vulnerabilities",
      "description": "Container images often contain vulnerable dependencies - scanning prevents deploying known CVEs. Install Trivy: 'curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh'. Scan images: 'trivy image nginx:1.21' shows all vulnerabilities with severity levels. Integrate scanning into CI/CD pipelines: fail builds with HIGH/CRITICAL vulnerabilities. Use admission controllers like OPA/Gatekeeper or Kyverno to block unscanned or vulnerable images at deployment time. For private registries, use Trivy with '--username' and '--password' flags or registry-specific tools (AWS ECR scanning, GCR vulnerability scanning, Harbor built-in scanner). Scan regularly - new CVEs are discovered daily. Set up automated scanning schedules. Update base images frequently and rebuild applications. Document exceptions for vulnerabilities that can't be immediately patched.",
      "critical": true
    },
    {
      "id": "resource-limits",
      "title": "Configure resource limits",
      "description": "Resource limits prevent noisy neighbor problems and DoS attacks. Every container should have requests (guaranteed resources) and limits (maximum resources). Set in pod specs: 'resources: {requests: {memory: \"256Mi\", cpu: \"200m\"}, limits: {memory: \"512Mi\", cpu: \"500m\"}}'. Requests affect scheduling - pods won't schedule if resources unavailable. Limits trigger throttling (CPU) or OOMKill (memory) if exceeded. Use LimitRanges to set namespace defaults: prevents forgetting limits on new deployments. Use ResourceQuotas to cap total namespace resource consumption. Monitor actual usage with 'kubectl top pods' or metrics-server. Start with generous limits, then optimize based on monitoring data. Missing limits allows containers to consume all node resources, starving other pods. For critical workloads, set requests=limits (Guaranteed QoS). Test memory limits carefully - OOMKills cause pod restarts.",
      "critical": false
    },
    {
      "id": "secrets-management",
      "title": "Use secrets management",
      "description": "Never hardcode secrets in manifests, images, or ConfigMaps. Use Kubernetes Secrets at minimum: 'kubectl create secret generic db-password --from-literal=password=mysecret'. However, Secrets are only base64-encoded by default, not encrypted at rest. Enable encryption at rest: configure kube-apiserver with '--encryption-provider-config' pointing to an EncryptionConfiguration file using aescbc or kms providers. Better yet, use external secret managers: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, Google Secret Manager. Tools like External Secrets Operator or Sealed Secrets sync external secrets into Kubernetes. Never commit secrets to Git - use tools like git-secrets or pre-commit hooks to prevent accidents. Use RBAC to restrict secret access: 'kubectl create role secret-reader --verb=get --resource=secrets --resource-name=db-password'. Rotate secrets regularly. For sensitive workloads, consider injecting secrets as volumes rather than environment variables (reduces exposure in process listings).",
      "critical": true
    },
    {
      "id": "enable-tls",
      "title": "Enable TLS for all endpoints",
      "description": "All cluster communication must be encrypted. Verify TLS between components: check kube-apiserver, etcd, kubelet all use TLS certificates. For etcd: '--cert-file', '--key-file', '--peer-cert-file', '--peer-key-file' flags. For kubelet: ensure '--tls-cert-file' and '--tls-private-key-file' are set. Rotate certificates before expiry: 'kubeadm certs renew all' for kubeadm clusters. For ingress traffic, use Ingress with TLS: store certificates in Secrets, reference in Ingress spec with 'tls:' section. Use cert-manager to automate certificate management with Let's Encrypt or internal CAs. For service mesh (Istio, Linkerd), enable mTLS: encrypts pod-to-pod traffic and provides identity-based auth. Verify TLS with: 'kubectl get cm -n kube-system kubeadm-config -o yaml' check advertiseAddress uses https. Monitor certificate expiration: 'kubeadm certs check-expiration'. Never disable TLS verification in production.",
      "critical": false
    },
    {
      "id": "regular-updates",
      "title": "Regular security updates",
      "description": "Kubernetes releases security patches regularly - staying current is critical. Subscribe to kubernetes-announce mailing list for security advisories. Follow Kubernetes version skew policy: control plane and nodes should be within 1-2 minor versions. Upgrade control plane first: 'kubeadm upgrade plan' shows available versions, 'kubeadm upgrade apply v1.28.0' upgrades. Use managed Kubernetes (EKS, GKE, AKS) for easier upgrade paths. Drain nodes before upgrading: 'kubectl drain node1 --ignore-daemonsets' moves workloads safely. Upgrade kubelet and kubectl on nodes after control plane. Test upgrades in non-production environments first. For components, update CNI plugins, CSI drivers, and ingress controllers regularly. Update container runtime (containerd, CRI-O): check for security patches. Maintain an upgrade schedule: aim for upgrades within 3-6 months of releases. Use 'kubectl version' to check current versions. Document upgrade runbooks and rollback procedures.",
      "critical": false
    }
  ]
}
