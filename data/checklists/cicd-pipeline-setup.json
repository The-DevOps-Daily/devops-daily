{
  "id": "cicd-pipeline-setup",
  "slug": "cicd-pipeline-setup",
  "title": "CI/CD Pipeline Setup Checklist",
  "description": "Complete guide to setting up a production-ready CI/CD pipeline.",
  "category": "DevOps",
  "difficulty": "intermediate",
  "estimatedTime": "1-2 hours",
  "tags": ["cicd", "pipeline", "automation", "devops", "testing"],
  "items": [
    {
      "id": "vcs-integration",
      "title": "Configure version control integration",
      "description": "Version control is the foundation of CI/CD. For GitHub: use GitHub Apps (recommended) or OAuth Apps for authentication - GitHub Apps provide granular permissions. In your CI/CD tool (Jenkins, GitLab CI, CircleCI, GitHub Actions), navigate to integrations/settings and add VCS connection. For GitHub Actions: workflows are stored in .github/workflows/ directory - no separate integration needed. For Jenkins: install GitHub plugin, create credentials (Personal Access Token with repo scope), configure GitHub server in Jenkins settings. For GitLab: CI/CD is built-in, ensure .gitlab-ci.yml exists in repository root. Use webhook triggers (not polling) for immediate pipeline execution on push/PR events. Set up branch protection rules: require PR reviews, passing CI checks before merge. Configure status checks to block merges if tests fail. Use deploy keys (read-only SSH keys) for cloning repositories rather than personal credentials. For private repositories, use secure credential storage (GitHub Secrets, Jenkins Credentials, AWS Secrets Manager). Enable signed commits for code integrity. Document repository structure and branching strategy (Git Flow, GitHub Flow, trunk-based development).",
      "critical": true
    },
    {
      "id": "automated-testing",
      "title": "Set up automated testing",
      "description": "Automated testing is the safety net of CI/CD. Follow the test pyramid: many unit tests (fast, isolated, test individual functions), fewer integration tests (test component interactions), minimal E2E tests (slow, brittle, test full user flows). For unit tests: use framework for your language (JUnit/Java, pytest/Python, Jest/JavaScript). Run on every commit: configure pipeline to run 'npm test', 'mvn test', 'pytest', etc. Aim for 80%+ code coverage. For integration tests: test database interactions, API contracts, message queues. Use test databases (Docker containers with test data) - never test against production. For E2E tests: use tools like Selenium, Cypress, Playwright. Run E2E tests on staging environments that mirror production. Parallelize tests to reduce execution time: most CI/CD platforms support test parallelization. Fail fast: order tests from fastest to slowest, stop pipeline on first failure. Generate test reports: use JUnit XML format for compatibility. Archive test results as CI artifacts. Use code coverage tools (JaCoCo, Istanbul, Coverage.py) and set minimum thresholds. Flaky tests are worse than no tests - fix or remove them immediately. Set up test data fixtures and tear down after tests to maintain consistency.",
      "critical": true
    },
    {
      "id": "code-quality-checks",
      "title": "Implement code quality checks",
      "description": "Code quality tools catch bugs, security issues, and maintainability problems before code review. For linting: use ESLint (JavaScript), Pylint/Flake8 (Python), RuboCop (Ruby), Checkstyle (Java). Configure in pipeline: 'npm run lint', 'pylint src/', 'rubocop'. Fail builds on errors, warn on style issues. For formatting: use Prettier (JavaScript), Black (Python), gofmt (Go) - enforce consistent style. Add pre-commit hooks to format code automatically before commits. For static analysis: use SonarQube/SonarCloud (multi-language, detects bugs, code smells, security vulnerabilities), SpotBugs (Java), Bandit (Python security), Brakeman (Rails security). Integrate SonarQube with pipeline: run 'sonar-scanner' after tests, set quality gates (minimum coverage %, max code smells, no critical bugs). Most tools generate reports - publish as pipeline artifacts. Configure IDE plugins to show issues during development (shift-left approach). Set up .editorconfig for consistent editor settings. Define coding standards document and enforce with tools. For complex codebases, add architecture validation (ArchUnit, dependency-cruiser) to prevent architectural violations. Review and adjust rules over time - balance strictness with developer productivity.",
      "critical": false
    },
    {
      "id": "security-scanning",
      "title": "Configure security scanning",
      "description": "Security scanning finds vulnerabilities before they reach production. SAST (Static Application Security Testing) analyzes source code: use SonarQube, Checkmarx, Veracode, or Semgrep. Run early in pipeline: 'semgrep --config=auto' scans for common vulnerabilities (SQL injection, XSS, hardcoded secrets). Dependency scanning checks for vulnerable libraries: use Snyk, OWASP Dependency-Check, npm audit, pip-audit, Dependabot. Run on every PR: 'npm audit --audit-level=high' fails builds with high/critical vulnerabilities. For Docker images, use Trivy, Clair, or Anchore: 'trivy image myapp:latest --exit-code 1 --severity HIGH,CRITICAL'. DAST (Dynamic Application Security Testing) tests running applications: use OWASP ZAP, Burp Suite. Run against deployed staging environments: 'zap-baseline.py -t https://staging.example.com'. Secret scanning prevents credential leaks: use GitGuardian, TruffleHog, git-secrets. Scan commits: 'trufflehog git file://. --since-commit HEAD~1'. Set up security gates: fail builds with critical vulnerabilities, require approval for high vulnerabilities. Generate security reports and track over time. Automate patching: Dependabot auto-creates PRs for dependency updates. Monitor security advisories for your tech stack. Have incident response plan for discovered vulnerabilities in production.",
      "critical": true
    },
    {
      "id": "deployment-automation",
      "title": "Set up deployment automation",
      "description": "Automated deployments eliminate human error and enable rapid releases. Define environments: dev (automatic on merge to develop), staging (automatic on merge to main), production (manual approval or automatic with safeguards). Choose deployment strategy: Rolling (gradual replacement, zero downtime), Blue-Green (two identical environments, instant switch), Canary (gradual traffic shift to new version). For Kubernetes: use Helm charts or Kustomize for configuration management. Deploy with: 'helm upgrade --install myapp ./chart --set image.tag=$VERSION'. Use namespaces per environment. For AWS: use CodeDeploy, ECS task definitions, or Elastic Beanstalk. Include health checks in deployment: check /health endpoint returns 200 before marking deployment successful. Implement progressive delivery: deploy to 1% of users, monitor metrics, gradually increase to 100%. Use feature flags (LaunchDarkly, Unleash) to decouple deployment from release - deploy anytime, enable features when ready. Configure deployment pipelines: staging should mirror production architecture. Use infrastructure as code (Terraform, CloudFormation) for reproducible environments. Tag deployments with version, commit SHA, timestamp. Send deployment notifications to Slack/Teams. Document rollback procedures clearly.",
      "critical": false
    },
    {
      "id": "rollback-procedures",
      "title": "Configure rollback procedures",
      "description": "Rollbacks save you when deployments go wrong. Automate rollback triggers: monitor error rates, latency, failed health checks after deployment. If metrics exceed thresholds, automatically roll back. For Kubernetes: use 'kubectl rollout undo deployment/myapp' to revert to previous version. Configure readiness/liveness probes: if new pods don't become ready within timeout, rollout automatically fails and rolls back. For AWS ECS: keep previous task definition versions, enable circuit breaker to auto-rollback failed deployments. For Blue-Green deployments: rollback is instant - just switch load balancer back to blue environment. For Canary deployments: halt traffic shift and route all to old version. Document manual rollback steps: exact commands, expected duration, verification steps. Test rollback procedures regularly - if you've never rolled back, you don't know if it works. Use versioned artifacts: keep Docker images, build artifacts, database schemas for all production versions. Implement database migration rollbacks: use tools that support down migrations (Flyway, Liquibase). Define rollback time windows: 'If we don't roll back within 30 minutes, proceed forward with hotfix instead'. Alert on-call team immediately when automated rollback occurs. Post-incident: analyze why deployment failed and improve deployment pipeline.",
      "critical": false
    },
    {
      "id": "monitoring-alerting",
      "title": "Enable monitoring and alerting",
      "description": "Monitor your CI/CD pipeline health to maintain velocity and catch issues early. Track pipeline metrics: build success rate (aim for >95%), average build duration (identify slowdowns), queue time (scaling indicator), test failure rate, deployment frequency (DORA metric). Use built-in dashboards: GitHub Actions Insights, GitLab CI/CD analytics, Jenkins monitoring plugin. For detailed monitoring: send pipeline metrics to Prometheus, Datadog, New Relic. Create alerts for: pipeline failures (notify team immediately via Slack/PagerDuty), long-running builds (>2x average duration), test flakiness (same test fails intermittently), security scan findings (critical vulnerabilities). Monitor deployment health post-deployment: track application metrics (error rate, latency, throughput) using APM tools (Datadog, New Relic, Prometheus+Grafana). Set up synthetic monitoring: continuously test critical user flows in production. Correlate deployments with metric changes: if error rate spikes after deployment, alert and consider rollback. Implement SLOs: 'Deployments should complete in <15 minutes 95% of the time', '99% of builds should succeed'. Use dashboards visible to entire team - pipeline health should be transparent. Review metrics in retrospectives: identify bottlenecks and optimize. Monitor infrastructure costs: track compute hours for CI runners, optimize to reduce costs.",
      "critical": false
    }
  ]
}
