{
  "id": "production-deployment",
  "slug": "production-deployment",
  "title": "Production Deployment Checklist",
  "description": "Pre-deployment checklist to ensure safe and successful production releases.",
  "category": "DevOps",
  "difficulty": "beginner",
  "estimatedTime": "20-30 minutes",
  "tags": ["deployment", "production", "release", "devops"],
  "items": [
    {
      "id": "review-code-changes",
      "title": "Review code changes",
      "description": "Code review is your last line of defense before production. Use pull request (PR) workflow: never commit directly to main/production branch. Require at least 2 reviewers for production changes (1 reviewer for non-critical changes). Reviewers should check: functionality (does code do what it's supposed to?), edge cases (error handling, null checks, boundary conditions), security (no hardcoded secrets, SQL injection, XSS vulnerabilities), performance (database query optimization, caching, algorithm efficiency), maintainability (clear variable names, comments for complex logic, follows team conventions). Use PR templates to ensure reviewers check critical items. Enable branch protection rules: require passing CI checks, require review approvals, dismiss stale reviews on new commits. For large changes, break into smaller PRs - easier to review thoroughly. Consider pair programming for complex features. Use automated tools to assist review: code coverage changes, static analysis results, security scan findings. Document 'why' in commit messages and PR descriptions, not just 'what'. For hotfixes: still require review, but establish expedited process. Track review turnaround time - long delays slow team velocity.",
      "critical": true
    },
    {
      "id": "run-all-tests",
      "title": "Run all tests",
      "description": "Never deploy if tests are failing - even one flaky test. Run full test suite locally before pushing: unit tests, integration tests, end-to-end tests. In CI/CD pipeline, ensure all test stages pass: verify green checkmarks on GitHub/GitLab. For large test suites, run in parallel to save time. Check code coverage hasn't decreased: most tools can compare coverage between branches. Run tests against a staging environment that mirrors production: same infrastructure, similar data volume, same configuration. Include smoke tests: quick checks of critical functionality (homepage loads, API health endpoint returns 200, database connection works, authentication works). For database-dependent tests: use test database with representative data, never test against production. If any test fails: fix the issue, don't disable the test or mark it as 'skip'. For E2E tests with UI: run in headless mode for speed, capture screenshots on failure for debugging. Check for test flakiness: run full suite 3-5 times - if any test fails inconsistently, investigate and fix. Run performance tests for high-traffic features: ensure response times meet SLAs under load. Verify tests run successfully in the exact environment configuration that will be deployed (same OS, runtime version, dependencies).",
      "critical": true
    },
    {
      "id": "update-documentation",
      "title": "Update documentation",
      "description": "Keep documentation in sync with code changes. Update README.md: new features should include usage examples, new dependencies should be listed with version requirements, changed configuration should document new environment variables or config files. Update API documentation: if you changed API contracts (request/response schemas, new endpoints, deprecated endpoints), update OpenAPI/Swagger specs, generate new API docs with tools like Swagger UI, Redoc, or Postman. Update CHANGELOG.md: follow Keep a Changelog format with sections: Added (new features), Changed (changes in existing functionality), Deprecated (soon-to-be removed features), Removed (now removed features), Fixed (bug fixes), Security (security updates). Version numbers should follow Semantic Versioning (MAJOR.MINOR.PATCH). Update architecture diagrams if you changed system design: infrastructure diagrams, sequence diagrams, ER diagrams. Update runbooks: if you changed deployment process, configuration, or operational procedures. For breaking changes: write migration guide helping users upgrade from previous version. Update inline code comments for complex logic. Check if wiki or Confluence pages need updates. Generate changelog automatically from commit messages: use conventional commits (feat:, fix:, docs:, etc.) and tools like semantic-release or standard-version. Documentation PRs should be part of feature PRs, not separate afterward.",
      "critical": false
    },
    {
      "id": "create-backup",
      "title": "Create backup",
      "description": "Backups are your insurance policy against deployment disasters. For databases: create manual snapshot/backup immediately before deployment. For PostgreSQL: 'pg_dump dbname > backup_$(date +%Y%m%d_%H%M%S).sql' or use pg_basebackup for full cluster backup. For MySQL: 'mysqldump --all-databases > backup_$(date +%Y%m%d_%H%M%S).sql' or use Percona XtraBackup for large databases. For managed databases (AWS RDS, Azure SQL): create manual snapshot in console - automated snapshots might not align with deployment timing. Verify backup completed successfully: check file size, test restore on dev environment. For stateful data: backup file storage (S3 buckets, EFS mounts) - use versioning or create point-in-time copies. Document backup location and restore procedure: future you (or your replacement) needs to know how to restore. Set retention policy: keep deployment-time backups for at least 30 days. For critical databases: test restore procedure regularly - untested backups are useless. Consider backup encryption for sensitive data. Backup application configuration: Docker images, Kubernetes manifests, environment variables, secrets. For zero-downtime deployments with database migrations: ensure migrations are backward-compatible (old code can run with new schema) - enables instant rollback without restoring database.",
      "critical": true
    },
    {
      "id": "notify-team",
      "title": "Notify team members",
      "description": "Communication prevents surprises and ensures support is available if things go wrong. Send deployment notification to: development team, DevOps/SRE team, product managers, support team, stakeholders. Use established channels: Slack deployment channel, Teams, email, or ticketing system. Include in notification: what's being deployed (version number, PR/issue links), when (scheduled deployment time, expected duration), who's deploying (name of engineer), changes included (high-level summary, link to changelog), rollback plan (how to rollback, who can authorize), support contacts (who to call if issues arise). For scheduled maintenance windows: notify customers 24-48 hours in advance via status page (Statuspage.io, custom page), in-app notifications, email. For zero-downtime deployments: quick notification to internal teams is sufficient. Coordinate with other teams: ensure no conflicting deployments, no load tests or security scans during deployment, on-call engineer is available. Use deployment tools that post to Slack automatically: include deployment status, success/failure, deployment artifacts. After deployment: send completion notification with summary (deployed successfully, any issues encountered, next steps). For major releases: schedule deployment during low-traffic hours (nights, weekends) and have team members on standby. Create deployment calendar visible to entire organization - avoid surprise deployments.",
      "critical": false
    },
    {
      "id": "deploy-staging",
      "title": "Deploy to staging first",
      "description": "Staging is your production dress rehearsal - use it to catch deployment issues before customers do. Staging environment should mirror production: same infrastructure (instance types, count), same configuration (environment variables, secrets), same data schema (use anonymized production data or realistic test data), same network setup (load balancers, DNS, SSL certificates), same monitoring and logging. Deploy exact same artifacts that will go to production: same Docker image tag, same build artifacts, same database migration scripts. Verify deployment process works: no configuration errors, no permission issues, no missing dependencies, correct version deployed. Run full test suite against staging: automated E2E tests, manual exploratory testing of new features, check critical user flows (login, checkout, data entry, reports). Test database migrations: ensure migrations run successfully, check migration rollback works, verify data integrity after migration. Test at production scale if possible: load testing with tools like k6, JMeter, Gatling - simulate realistic traffic patterns. Check monitoring dashboards: metrics appear correctly, logs are flowing, alerts don't trigger falsely. Leave staging deployed for sufficient time to catch time-based issues: scheduled jobs, cron tasks, time-zone handling. For blue-green deployments: staging is your 'blue' environment - validate thoroughly before switching production traffic. If anything fails in staging: fix issues, redeploy to staging, verify again - never skip to production.",
      "critical": true
    },
    {
      "id": "verify-monitoring",
      "title": "Verify monitoring and logging",
      "description": "You can't fix what you can't see - verify observability before deployment. Check monitoring systems are operational: Datadog, New Relic, Prometheus, CloudWatch dashboards loading correctly and showing data. Verify application metrics: request rate, error rate, latency (p50, p95, p99), throughput. Create or update deployment dashboard showing: current version deployed, deployment timeline, key metrics (errors, latency), recent logs. Check log aggregation: logs flowing to centralized logging (Elasticsearch, Splunk, CloudWatch Logs, Datadog Logs). Verify log levels are appropriate: INFO for normal operations, WARN for issues that don't break functionality, ERROR for failures. Include structured logging with context: request IDs, user IDs, timestamps, environment. Set up deployment markers: annotate monitoring dashboards when deployment happens - makes correlation easy when metrics change. Configure alerts before deployment: alert on error rate spike (>5% increase), latency spike (p95 >1.5x baseline), pod/instance failures, database connection pool exhaustion. Test alerts actually fire: send test alert to Slack/PagerDuty - verify notifications reach on-call engineer. Enable distributed tracing for complex systems: Jaeger, Zipkin, AWS X-Ray help debug issues across microservices. Document where to find logs and metrics: onboarding new team members or incident response needs quick access. Set up log retention: balance between cost and compliance requirements.",
      "critical": false
    },
    {
      "id": "smoke-tests",
      "title": "Perform smoke tests",
      "description": "Smoke tests are your first indicator of deployment success - they verify critical functionality works. Immediately after deployment, run: health check endpoint (GET /health should return 200 with status: ok), homepage loads (verify no 500 errors, no broken CSS/JS), authentication works (login with test user, verify JWT/session creation), database connectivity (query returns expected data), critical API endpoints (test 3-5 most important endpoints), background jobs running (check last job execution timestamp). For web applications: click through critical user flows (registration, login, main feature, logout). For APIs: run Postman collection or curl commands against production endpoints. Check external integrations: payment gateway test transaction, email service sends test email, third-party API calls succeed. Verify static assets load: check CDN serving images/CSS/JS correctly, verify correct version of frontend assets deployed. Monitor application logs during smoke tests: look for errors, exceptions, warnings that didn't appear in staging. Check metrics dashboards: error rate should be 0% or baseline, latency should match pre-deployment levels. Run smoke tests from multiple locations: different geographic regions, different user types (free vs. paid users). Automate smoke tests: create script that runs immediately post-deployment, integrate with deployment pipeline for automatic verification. Document smoke test checklist: ensure consistency across deployments and different team members. If any smoke test fails: investigate immediately, consider rollback if critical functionality broken.",
      "critical": true
    },
    {
      "id": "update-runbooks",
      "title": "Update runbooks",
      "description": "Runbooks help team members respond to incidents and perform operational tasks consistently. Update deployment runbook if process changed: new steps added, different tools used, changed configuration, modified rollback procedure. Create or update incident response runbooks for new features: 'How to debug feature X', 'What to do if Y fails', 'How to manually trigger Z'. Include in runbooks: problem description (symptoms, error messages), investigation steps (logs to check, metrics to review, commands to run), resolution steps (exact commands with parameters, expected output, validation steps), escalation path (who to contact if steps don't work). Document common issues and fixes: 'If database migration times out, do X', 'If memory usage spikes, check Y'. Use consistent format: title, overview, prerequisites, step-by-step instructions with code blocks, troubleshooting section, related runbooks. Store runbooks where engineers can find them: wiki, Confluence, GitHub repo (docs/runbooks/), PagerDuty linked runbooks. Include version information: 'This runbook applies to v2.x and later', 'Updated for Kubernetes migration'. Link runbooks to monitoring alerts: when alert fires, link directly to relevant runbook for faster resolution. Review runbooks quarterly: remove outdated procedures, add lessons learned from recent incidents, ensure commands still work. For new team members: runbooks serve as operational training material. Consider runbook templates for consistency: deployment runbook template, incident response template, maintenance procedure template.",
      "critical": false
    }
  ]
}
